
# 🧠 My ML Journey — Step 2: Gathering Data 🚀

Welcome to my repository!  
This is not just random code — it’s part of my **Machine Learning Development Life Cycle (MLDLC)** journey.  
I’m building my skills **step by step**, and right now I’m focused on **Step 2: Gathering Data**.  

---

## 🔄 Where I Am in the ML Workflow

✅ Step 1 — Framing the Problem  
   I explored how to define ML problems properly.  

🟢 Step 2 — Gathering Data (current focus)  
   Here I’m learning all the different ways data can be collected:  
   - flat files (CSV, JSON)  
   - databases (SQL, SSMS)  
   - APIs  
   - and even scraping from the web.  

⏳ Step 3 — Data Cleaning & Preparation (coming next...)  

---

## 📅 My Daily Practice Log

- **Day 1:** Played with **CSV** files → reading, writing, analyzing.  
- **Day 2:** Combined **JSON parsing** with fetching data from my own **SQL Server (SSMS database)**.  
- **Day 3:** Wrote queries in **SQL using Python** → storing and retrieving data programmatically.  
- **Day 4:** Pulled real-time data by **Fetching APIs**.  
- **Day 5:** Tried **Web Scraping** with BeautifulSoup to collect data directly from websites.  

---

## 🛠️ Tools in My Toolkit
- Python 3  
- Pandas  
- JSON  
- SQL (SQLite3 + SSMS)  
- Requests  
- BeautifulSoup4  

---

## 🎯 Why Am I Doing This?
Because before you can clean, model, or deploy —  
you must **master the art of gathering data**.  

This repo is my sandbox where I practice data collection in multiple formats, making myself ready for the tougher ML steps ahead.  

---

## 📌 Run the Code
1. Clone the repo:
   ```bash
   git clone https://github.com/Sidra-009/your-repo-name.git
````

2. Move into the folder:

   ```bash
   cd your-repo-name
   ```
3. Run the practice files step by step:

   ```bash
   python day1_csv.py
   ```

---

## 🌟 Coming Soon

* Data cleaning & preprocessing
* Feature engineering
* First ML models 🚀

---

✍️ *This is my personal ML learning diary in code form. If you’re also on this journey, welcome aboard!*

```
